{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1IXwkk3Op4kqbdavEBmIcg00fDiC3omll",
      "authorship_tag": "ABX9TyPLsrMDBeqjASmvb4139cXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonghhhh/lecture_colabs/blob/main/%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91_%EB%84%A4%EC%9D%B4%EB%B2%84%EB%89%B4%EC%8A%A4%EB%B3%B8%EB%AC%B8_110924.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfBL3REzlisM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 네이버 뉴스 본문 수집"
      ],
      "metadata": {
        "id": "6DeXezpbPGCK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENXn4cMwYcaa"
      },
      "source": [
        "## 패키지 불러오기: urllib, BeautifulSoup, pandas\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80MQGJ51iK76"
      },
      "source": [
        "from urllib.request import urlopen   # url에 접속\n",
        "from bs4 import BeautifulSoup   # html을 구조적으로 나누고(parsing), 원하는 부분을 추출하는(select) 기능\n",
        "\n",
        "import pandas as pd             # 수집된 내용을 dataframe 방식으로 저장\n",
        "import pprint                   # 정리해서 보여주기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 함수 만들기: 네이버 뉴스 url로 본문 수집"
      ],
      "metadata": {
        "id": "Y0UbH4ELPdcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "more_remove={\"KBS\": \"■ 제보하기▷ 전화 : 02-781-1234, 4444▷ 이메일 : kbs1234@kbs.co.kr▷ 카카오톡 : 'KBS제보' 검색, 채널 추가▷ 네이버, 유튜브에서 KBS뉴스를 구독해주세요!\",\n",
        "\"MBC\": \"MBC 뉴스는 24시간 여러분의 제보를 기다립니다. ▷ 전화 02-784-4000▷ 이메일 mbcjebo@mbc.co.kr▷ 카카오톡 @mbc제보\",\n",
        "\"YTN\" : \"※ '당신의 제보가 뉴스가 됩니다'[카카오톡] YTN 검색해 채널 추가[전화] 02-398-8585[메일] social@ytn.co.kr\"}\n",
        "\n",
        "def text_collect(url):  # 네이버뉴스 url = 'https://n.news.naver.com/article/214/0001327361?type=main'\n",
        "    response = requests.get(url)\n",
        "    time.sleep(0.3)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    try:\n",
        "        writer=soup.select('span.byline_s')[0].text\n",
        "    except:\n",
        "        writer = ''\n",
        "    try:\n",
        "        txt=soup.select('#dic_area')[0].text\n",
        "        for rem in [remove.text for remove in soup.select('em.img_desc')]+[writer]:\n",
        "            txt=txt.replace(rem, '')\n",
        "        txt=txt.replace('\\t', '').strip()\n",
        "        if 'article/056' in url: txt=txt.replace(more_remove['KBS'], '').strip()\n",
        "        elif 'article/214' in url: txt=txt.replace(more_remove['MBC'], '').strip()\n",
        "        elif 'article/052' in url: txt=txt.replace(more_remove['YTN'], '').strip()\n",
        "        else: pass\n",
        "    except:\n",
        "        txt=''\n",
        "    try:\n",
        "        upload_date=soup.select('span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')[0].text\n",
        "    except:\n",
        "        upload_date=''\n",
        "    try:\n",
        "        section=soup.select('em.media_end_categorize_item')[0].text\n",
        "    except:\n",
        "        section = ''\n",
        "    return [url, upload_date, section, writer, txt]    # 입력날짜시간, 뉴스섹션, 작성자, 본문 출력"
      ],
      "metadata": {
        "id": "IrBWjLsFy9as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실습과 저장"
      ],
      "metadata": {
        "id": "tr8lHp52T1VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습(1)\n",
        "url='https://n.news.naver.com/mnews/article/008/0005016916?sid=101'\n",
        "text_collect(url)"
      ],
      "metadata": {
        "id": "ZeoPOIIky9Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습(2): 복수의 기사 본문 수집\n",
        "result_df=pd.read_excel('/content/drive/MyDrive/제주43/result_KBS.xlsx')\n",
        "result_df.info()\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "texts=[]\n",
        "for url in tqdm(list(result_df['url_naver'])[:]):     # 앞의 result_df에서 수집한 url 불러와 수집\n",
        "    texts.append(text_collect(url))\n",
        "    time.sleep(0.5)\n",
        "result_text=pd.DataFrame(texts, columns=['url_naver', 'upload_date', 'section', 'writer', 'full_text'])  # 수집 결과를 dataframe으로 구성\n",
        "\n",
        "result_all=pd.merge(result_df, result_text, on='url_naver', how='left')    # result_df와 result_text를 url 기준으로 결합"
      ],
      "metadata": {
        "id": "Ho8IP_3Ay9Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장\n",
        "\n",
        "result_all.to_excel('/content/drive/MyDrive/제주43/result_KBS_본문.xlsx')  # 자신의 작업 폴더에 저장"
      ],
      "metadata": {
        "id": "Ae_EzEBHQXD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}