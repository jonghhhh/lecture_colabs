{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1EcW5iT4If1_HnAPmbxbAyDqjUy7B-e3c","authorship_tag":"ABX9TyPxL0xyyw3iTimdd9iTgjVZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 텍스트 감성분석\n","-  Kcbert 기반 전이학습\n","- 긍정(1) 부정(0) 분류기 개발과 추론\n","- 네이버 영화 리뷰 데이터(nsmc): https://huggingface.co/datasets/nsmc"],"metadata":{"id":"Dw9oBsVBAW07"}},{"cell_type":"markdown","source":["### 페키지 설치와 불러오기"],"metadata":{"id":"w0-Rxn1Kj0KY"}},{"cell_type":"code","source":["!pip install accelerate -U"],"metadata":{"id":"yrZPVbXPnprB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import numpy as np"],"metadata":{"id":"G4R1cEyrAm7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 데이터셋 구조 설정"],"metadata":{"id":"G6wm9ziGj9Y8"}},{"cell_type":"code","source":["# 데이터셋 클래스 정의\n","class NSMCDataset(Dataset):\n","    def __init__(self, file_path, tokenizer, max_length):\n","        self.data = pd.read_excel(file_path)[['id','text','label']]  # 파일 불러오기\n","        self.data.dropna(inplace=True)\n","        self.data = self.data[self.data['text'].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n","        self.tokenizer = tokenizer  # 토크나이저 초기화\n","        self.max_length = max_length  # 입력 시퀀스의 최대 길이 설정\n","\n","    def __len__(self):\n","        return len(self.data)  # 데이터셋의 길이 반환\n","\n","    def __getitem__(self, idx):\n","        text = self.data.iloc[idx, 1]  # 데이터프레임에서 텍스트 추출\n","        label = self.data.iloc[idx, 2]  # 데이터프레임에서 라벨 추출\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        input_ids = torch.tensor(inputs['input_ids'], dtype=torch.long)  # 토큰 ID 텐서로 변환\n","        attention_mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)  # 어텐션 마스크 텐서로 변환\n","        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)  # 토큰 타입 ID 텐서로 변환\n","        label = torch.tensor(label, dtype=torch.long)  # 라벨 텐서로 변환\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'token_type_ids': token_type_ids,\n","            'labels': label\n","        }  # 딕셔너리 형태로 반환"],"metadata":{"id":"lGFdcj2MArBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실습데이터: 데이터의 일부만 가지고 연습. 실전에서는 전체 데이터 사용\n","data=pd.read_excel(\"/content/drive/MyDrive/2024_1_class/nsmc_ratings.xlsx\")\n","label_0 = data[data['label'] == 0].sample(n=5000, random_state=42)\n","label_1 = data[data['label'] == 1].sample(n=5000, random_state=42)\n","small_data = pd.concat([label_0, label_1]).reset_index(drop=True)\n","small_data.to_excel(\"/content/drive/MyDrive/2024_1_class/nsmc_ratings_small.xlsx\")"],"metadata":{"id":"jqJd-5mNAq-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# huggingface에 데이터셋이 있을 경우: https://huggingface.co/datasets/nsmc\n","! pip install datasets\n","\n","from datasets import load_dataset\n","dataset_hf = load_dataset('nsmc')  # 로드\n","dataset_hf=pd.concat([pd.DataFrame(dataset_hf['train']), pd.DataFrame(dataset_hf['train'])])\n","dataset_hf=dataset_hf.drop_duplicates()"],"metadata":{"id":"nK-DkOG-lj7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터셋 불러오기와 분리: 훈련(train)-검증(validation)-평가(test)"],"metadata":{"id":"HI0nUWUekGxl"}},{"cell_type":"code","source":["# 데이터셋 설정 및 분리\n","tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')  # 토크나이저 로드\n","dataset = NSMCDataset(\"/content/drive/MyDrive/2024_1_class/nsmc_ratings_small.xlsx\", tokenizer, max_length=64)  # 데이터셋 불러오기\n","train_size = int(0.8 * len(dataset))  # 훈련 데이터 크기\n","val_size = int(0.1 * len(dataset))  # 검증 데이터 크기\n","test_size = len(dataset) - train_size - val_size  # 테스트 데이터 크기\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])  # 데이터셋 분리\n","\n","# 데이터 로더 설정\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # 훈련 데이터 로더\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # 검증 데이터 로더\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # 테스트 데이터 로더"],"metadata":{"id":"mNS35IzXAq7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 베이스 모델 설정"],"metadata":{"id":"APH8K0rkkhJZ"}},{"cell_type":"code","source":["# BERT 모델 로드 및 드롭아웃 설정\n","config = BertConfig.from_pretrained(\"beomi/kcbert-base\", num_labels=2)\n","config.hidden_dropout_prob = 0.1  # 드롭아웃 확률 설정\n","config.attention_probs_dropout_prob = 0.1  # 어텐션 드롭아웃 확률 설정\n","\n","model = BertForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", config=config)\n","\n","# 모델을 GPU로 이동\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"metadata":{"id":"wMWgldb1AyxM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습: 훈련(train)-검증(validation)*이탤릭체 텍스트* 데이터 활용"],"metadata":{"id":"c-FlPV9akpKM"}},{"cell_type":"code","source":["# 훈련 인자 설정\n","training_args = TrainingArguments(\n","    output_dir='./results/',  # 결과 저장 폴더\n","    num_train_epochs=1,  # 에포크 수\n","    per_device_train_batch_size=32,  # 훈련 배치 크기\n","    per_device_eval_batch_size=32,  # 평가 배치 크기\n","    warmup_steps=100,  # 학습 초기 100단계 동안 학습률을 선형적으로 증가시키고, 이후에는 설정된 학습률 유지\n","    weight_decay=0.01,  # 모델 가중치를 0.01의 비율로 감소시켜 모델이 과적합되지 않도록 도와줌\n","    logging_dir='./logs/',  # 로그 저장 폴더\n","    logging_steps=10,  # 로그 기록 단계\n","    evaluation_strategy=\"epoch\",  # 에포크 단위로 평가\n","    save_strategy=\"epoch\",  # 에포크 단위로 모델 저장\n","    #evaluation_strategy=\"steps\",  # 스텝 단위로 평가\n","    #eval_steps=100,  # 100 스텝마다 평가\n","    #save_strategy=\"steps\",  # 스텝 단위로 모델 저장\n","    #save_steps=100,  # 100 스텝마다 저장\n","    load_best_model_at_end=True,  # 최적 모델 로드\n","    metric_for_best_model=\"accuracy\",  # 최적 모델 기준\n","    greater_is_better=True,  # 높은 값이 더 나은지 여부\n","    save_total_limit=3,  # 최대 체크포인트 저장 수\n","    learning_rate=5e-5,  # 학습률\n","    optim=\"adamw_torch\"  # 옵티마이저\n",")\n","\n","# 평가 메트릭 정의\n","def compute_metrics(pred):\n","    labels = pred.label_ids  # 실제 라벨\n","    preds = pred.predictions.argmax(-1)  # 예측값\n","    acc = accuracy_score(labels, preds)  # 정확도 계산\n","    f1 = f1_score(labels, preds, average='weighted')  # F1 스코어 계산\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","    }"],"metadata":{"id":"k__BRjKUAyuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 모델 훈련\n","\n","# 트레이너 초기화\n","trainer = Trainer(\n","    model=model,  # 모델 설정\n","    args=training_args,  # 훈련 인자 설정\n","    train_dataset=train_dataset,  # 훈련 데이터셋\n","    eval_dataset=val_dataset,  # 검증 데이터셋\n","    compute_metrics=compute_metrics,  # 평가 메트릭 설정\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # 조기 종료 콜백 추가\n",")\n","\n","# 모델 훈련\n","trainer.train()"],"metadata":{"id":"c3hnrOBlAyrp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 평가(test)"],"metadata":{"id":"Unjbm3Ssk8ZB"}},{"cell_type":"code","source":["### 모델 평가\n","predictions, labels, _ = trainer.predict(test_dataset)  # 예측 수행\n","preds = np.argmax(predictions, axis=1)  # 예측값 추출\n","acc = accuracy_score(labels, preds)  # 정확도 계산\n","f1 = f1_score(labels, preds, average='weighted')  # F1 스코어 계산\n","cm = confusion_matrix(labels, preds)  # 혼동 행렬 계산\n","print(f\"Accuracy: {acc}\")\n","print(f\"F1 Score: {f1}\")\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# 모델과 토크나이저를 저장할 디렉토리\n","save_directory = '/content/drive/MyDrive/2024_1_class/nsmc_best_model/'\n","\n","# 모델과 토크나이저 저장\n","model.save_pretrained(save_directory)\n","tokenizer.save_pretrained(save_directory)"],"metadata":{"id":"XVaytr82nDsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 추론(inference): 모델 로드와 분석 데이터 적용\n"],"metadata":{"id":"HiXK_CVUlLnP"}},{"cell_type":"code","source":["# 추론 설정\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from safetensors.torch import save_file, load_file\n","\n","# 데이터셋 정의\n","class TextDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_length):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        inputs = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        return inputs['input_ids'].squeeze(), inputs['attention_mask'].squeeze()\n","\n","# 미니배치 추론 함수\n","def batch_inference(model, tokenizer, texts, batch_size=32, max_length=64):\n","    dataset = TextDataset(texts, tokenizer, max_length)\n","    dataloader = DataLoader(dataset, batch_size=batch_size)\n","\n","    model.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for input_ids, attention_mask in dataloader:\n","            input_ids = input_ids.to(model.device)\n","            attention_mask = attention_mask.to(model.device)\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            batch_predictions = torch.argmax(logits, dim=-1)\n","            predictions.extend(batch_predictions.cpu().numpy())\n","\n","    return predictions\n","\n","# KC BERT 모델과 토크나이저 로드\n","model_name = \"beomi/kcbert-base\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","\n","# safetensors 형식으로 저장된 모델 가중치 로드\n","state_dict = load_file('/content/drive/MyDrive/2024_1_class/nsmc_best_model/model.safetensors')\n","\n","# KC BERT 모델 구조 초기화 및 가중치 로드\n","loaded_model = BertForSequenceClassification.from_pretrained(model_name)\n","loaded_model.load_state_dict(state_dict)"],"metadata":{"id":"kaGAr-T0ptxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추론 실습\n","\n","texts = [\"이 영화 정말 재미있어요!\", \"이 영화는 실망 그 자체입니다.\", \"훌륭한 인물이 되어 사회에 기여할 겁니다.\", \"잘못한 일에 대해 반성하고 사과하세요.\"]\n","\n","# 모델을 GPU로 이동 (가능한 경우)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","loaded_model.to(device)\n","\n","# 미니배치 추론 수행\n","predicted_labels = batch_inference(loaded_model, tokenizer, texts, batch_size=2, max_length=128)\n","\n","for text, label in zip(texts, predicted_labels):\n","    print(f\"Text: {text} => Predicted label: {label}\")"],"metadata":{"id":"HMRWcr7YBIlr"},"execution_count":null,"outputs":[]}]}