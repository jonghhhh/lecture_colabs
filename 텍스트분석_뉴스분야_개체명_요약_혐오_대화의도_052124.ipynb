{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "13B2LI89Iv6hRDY9oVHuda4XdMeBIdUSR",
      "authorship_tag": "ABX9TyOnUTy/ufO5rEW/qfOQgauN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonghhhh/lecture_colabs/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D_%EB%89%B4%EC%8A%A4%EB%B6%84%EC%95%BC_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9A%94%EC%95%BD_%ED%98%90%EC%98%A4_%EB%8C%80%ED%99%94%EC%9D%98%EB%8F%84_052124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글 텍스트 분석: 뉴스분야-개체명-요약-혐오글-대회의도 등  \n",
        "- huggingface model: https://huggingface.co/models\n",
        "- 한국언론진흥재단의 빅카인즈랩: https://github.com/KPF-bigkinds/BIGKINDS-LAB/tree/main"
      ],
      "metadata": {
        "id": "-X0ir0SC2t_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분석용 사례"
      ],
      "metadata": {
        "id": "83dpA-XNVZ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article1 = '''쓰러진 행인 머리 짓밟고 발로 차고  집단폭행.\n",
        "울산 도심 한복판에서 조폭 추종세력 5명이 행인들을 집단 구타하는 영상이 온라인상에 공개돼 공분을 사고 있다. 경찰은 가해 남성 5명을 전원 검거해 조사 중이다.\n",
        "울산경찰청은 18일 A씨 등 20대 남성 5명을 폭력행위 등 처벌에 관한 법률 위반 혐의로 검거해 조사하고 있다고 밝혔다.\n",
        "사건은 지난 15일 오전 4시20분께 울산시 남구 삼산동의 한 번화가에서 발생했다.\n",
        "공개된 영상을 보면 남성 3명이 길바닥에 쓰러진 남성의 머리, 어깨 등을 발로 찼다. 피해 남성은 두 손으로 머리를 감싸고 있었고, 슈트를 입은 남성이 손으로 가해 남성들을 말렸다. 또 다른 남성 2명도 쓰러진 다른 남성의 얼굴을 향해 발길질을 하는 등 무차별 폭행을 가했다. 그 옆에는 또 다른 피해 남성이 쓰러져 있었다.\n",
        "사건 발생 직후, 신고를 받은 경찰이 현장에 출동했지만 가해 남성들은 이미 사라지고 난 후였다. 경찰은 17일 체포영장을 발부받아 가해 남성 5명을 검거했다.\n",
        "경찰 조사 결과, 이들의 폭행은 “길을 비켜주지 않는다”는 사소한 시비에서 시작된 것으로 드러났다. 이들은 관리 대상 조폭은 아니지만, 울산의 한 조직폭력 세력을 따르는 신흥 세력인 것으로 전해졌다. 경찰은 A씨 등 일행에 대해 구속영장을 신청하는 방안을 검토 중이다.\n",
        "무차별 폭행으로 피해를 입은 남성들은 온몸 타박상 등 전치 2주 상해를 입은 것으로 전해졌다. 영상을 본 네티즌들은 “절대 봐주지 마라”, “머리만 저렇게 집중적으로 폭행하면 큰 일 나는데..”, “한 명 쓰러뜨리고 저렇게 집단 폭행하는 건 뭐냐”, “저런 사람들 엄정하게 처벌해야 된다” 등의 반응을 보였다.\n",
        "'''\n",
        "# 제3조 ⑥항 선정보도 금지  주의조치 (022년 5월 19일)"
      ],
      "metadata": {
        "id": "4D3jLctkj34w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article2 = '''경찰, 6월까지 강절도·폭력 범죄 등 민생침해범죄 집중단속.\n",
        "경찰청 국가수사본부는 4월 1일부터 6월 30일까지 시민의 일상을 위협하는 강절도와 생활 주변 폭력 범죄에 대해 집중단속을 한다고 31일 밝혔다.\n",
        "이번 집중단속은 그간 감소하고 있던 강절도 범죄와 흉기 사용 범죄가 최근 다시 증가 추세로 돌아서 시행됐다.\n",
        "중점 단속 대상으로는 ‘강절도·장물 사범’, 흉기 사용 폭력이나 의료 현장 폭력 등 ‘생활 주변 폭력 사범’이다.\n",
        "경찰은 관서 간 유기적 공조를 바탕으로 강·절도 범죄 사건을 조기에 해결해 추가 피해 확산을 막을 계획이다.\n",
        "상습적 범죄는 여죄까지 수사해 구속하는 등 엄정 대응하고, 피해품 처분·유통 경로를 추적해 실질적인 피해 보상이 이뤄지도록 한다.\n",
        "흉기를 사용한 폭력 행위는 구속 수사를 원칙으로 하되 불구속 시에도 이상 동기 범죄 여부에 대한 분석과 정신질환 이력을 파악해서 응급입원 등 분리 조치 필요성을 적극적으로 검토할 예정이다.\n",
        "최근 의료계 집단행동으로 현장에 남은 의료 종사자의 업무 부담이 가중되는 상황에서 의료진과 119구급대원에 대한 폭력 행위는 폭행·협박·업무방해·공무집행방해 등 상황에 따른 혐의점을 자세히 조사해 조치하기로 했다.\n",
        "경찰청 관계자는 “민생침해범죄 근절을 위해선 지역 주민의 협력이 가장 중요하다”며 “주변의 범죄 행위를 발견하면 경찰에 적극적으로 제보해달라”고 당부했다.\n",
        "'''"
      ],
      "metadata": {
        "id": "GUscne7Xj31k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article3 = '''풍부한 첫 느낌, 깔끔한 여운…프리미엄 커피 라이프 ‘카누 원두커피’.\n",
        "최근 커피를 즐기는 인구가 늘어나면서 프리미엄 원두커피에 대한 수요가 증가하고 있다. 지난해 대한민국 대표 커피전문기업 동서식품이 선보인 ‘카누(KANU) 원두커피’는 원두 본연의 맛과 향을 즐길 수 있는 제품으로 총 6종(홀빈 3종·분쇄 원두 3종)을 갖춰 취향대로 골라 마실 수 있다.\n",
        "카누 원두커피 6종은 ‘FIRST BOLD, LAST CLEAN(풍부한 첫 느낌, 깔끔한 여운)’이라는 콘셉트로 입안 가득 퍼지는 풍부하고 진한 풍미와 목 넘김 후에 느껴지는 깔끔한 맛을 구현한 것이 특징이다.\n",
        "중남미와 아프리카 지역의 프리미엄 원두를 사용하고 원두마다 최적의 로스팅 프로파일을 적용해 고유의 풍미와 개성을 극대화했다.\n",
        "▶카누 실키 베이지 ▶카누 크리미 버건디 ▶카누 벨베티 블랙 등 3가지 플레이버를 각각 분쇄 원두(Ground Coffee)와 홀빈(Whole Bean: 분쇄하지 않은 원두) 2가지 타입으로 선보여 소비자 선택의 폭을 넓혔다. 제품 타입에 따라 에스프레소 방식이나 핸드드립, 콜드브루 등 다양한 방식으로 즐길 수 있다.\n",
        "‘카누 실키 베이지’는 원두를 라이트 로스팅해 과일류에서 느껴지는 매력적인 산미가 특징이다. ‘카누 크리미 버건디’는 미디엄 로스트 원두로 견과류의 진하고 고소한 풍미와 적당한 산미를 느낄 수 있고, ‘카누 벨베티 블랙’은 다크 로스트 원두의 묵직한 무게감 속에 스모키한 향과 달콤하고 쌉싸름한 풍미가 돋보인다.\n",
        "또한 동서식품은 최근 대용량 신제품 ‘카누 원두커피 카페 블렌드 다크로스팅’(1.13kg)을 론칭하고 전국 코스트코 매장에 입점했다. 카누 카페 블렌드 다크로스팅은 홀빈 타입으로 커피전문점을 운영하는 사업자나 대용량 원두를 원하는 소비자들을 위해 출시한 제품이다.\n",
        "동서식품은 지난 6일과 7일 이틀간 서울 용산구 한남동의 맥심 브랜드 체험공간 ‘맥심플랜트’에서 월드 바리스타 챔피언 엄보람 바리스타와 함께 ‘카누 원두커피 컬래버레이션 이벤트’를 진행했다.\n",
        "이번 이벤트는 카누 프리미엄 원두커피 ‘카페 블렌드’ 론칭과 함께 카누 원두커피의 우수한 품질을 알리기 위해 기획됐다. 행사에 참여한 엄보람 바리스타는 지난 2023년 월드 바리스타 챔피언십(WBC)에서 우승한 세계적인 바리스타다.\n",
        "‘카누 원두커피 컬래버레이션 이벤트’에서는 엄보람 바리스타가 직접 ‘카누 카페 블렌드 다크 로스팅’과 ‘카누 실키 베이지’ 원두를 활용한 핸드드립과 콜드브루 레시피를 소개하는 원데이 클래스와 함께 방문객 대상 샘플링과 시음행사가 진행됐다.\n",
        "동서식품 김민수 마케팅 매니저는 “카누 원두커피는 프리미엄 원두커피로 산지별 최고 수준의 원두만을 사용했으며 풍부한 첫맛과 깔끔한 여운을 갖춰 원두커피 입문자부터 애호가까지 모두 만족하게 할 수 있는 제품”이라며 “카누 원두커피와 함께 프리미엄 커피 라이프를 즐겨보길 바란다”고 말했다.'''\n",
        "# 제1조②항 사회·경제 세력으로부터의 독립과 제3조⑥항 보도자료 검증으로 주의 조치"
      ],
      "metadata": {
        "id": "0ZyCqvas7RhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article4 = ''''원두값 급등에 커피가격 올려야…동네카페 걱정'.\n",
        "국제 원두 가격이 급등한 가운데 국내 카페 자영업자들의 시름도 깊어지고 있다. 사진은 광주 한 카페의 메뉴판.\n",
        "“저같이 개인이 운영하는 카페는 저렴하게 판매해야만 손님이 오는데 최근 들어 거래처에서 원두 소매값을 올린다는 말이 있어 불안합니다.”\n",
        "광주 동구 학원가에서 소규모 개인 카페를 운영 중인 김모(52)씨는 요즘 아이스아메리카노 가격을 올릴지 고민 중이다. 최근 급격히 오른 원두 가격에 도매상들이 하나둘 가격을 올리기 시작하면서 가격 조정이 필요하기 때문이다.\n",
        "김씨는 “아직 거래처에서 원두 가격을 올리지 않았지만, 곧 올린다는 소문이 돈다. 개인 카페이다 보니 저렴한 가격에 커피를 판매하고 있는데 우윳값도 이미 많이 올라 여기에 원두값까지 오르면 매우 힘들어진다”며 “주변에 워낙 카페가 많다 보니 가격을 올리게 되면 가격 경쟁력이 떨어져 고민된다. 마지막까지 심사숙고해 가격을 올릴지 결정할 예정이다”고 한숨을 내쉬었다.\n",
        "시원한 커피 수요가 늘어나는 여름철이 다가오지만 카페 자영업자들은 비상이다. 이상기후로 커피 생산량이 급감하면서 원두값이 고공행진을 기록하며 도·소매상들이 원두 가격을 올린 까닭이다.\n",
        "7일 한국농수산식품유통공사(aT) 식품산업통계정보에 따르면 런던국제금융선물거래소(LIFFE) 기준 로부스타 원두 1톤 가격은 올해 평균 3427.19달러로 지난해(2492.82달러) 대비 37.48% 급증했다. 기존 1000달러~2000달러 사이를 유지하던 로부스타 가격은 올해 1월 3236.5달러로 치솟으며 전년동월(1962.86달러) 대비 무려 64.89% 뛰었다. 원두값은 꾸준히 증가 곡선을 그리며 지난 4월에는 3938.86달러 기록하며 16년 만에 최고가를 경신했다.\n",
        "원두가격 상승은 세계 로부스타 공급량의 3분의1을 차지하는 베트남에 가뭄이 지속되면서 원두 생산량이 급감한 것이 주된 요인이다. 로부스타 원두는 저렴한 가격으로 인스턴트 커피, 에스프레소 등 다양한 커피 원두에 혼합해 사용하는 제품인 만큼 국내 자영업자들에 타격이 큰 편이다.\n",
        "광주·전남 지역 카페에 원두를 제공하는 도매업체 관계자 최모(49)씨는 “국제 원두 가격이 오르면서 국내 원두값도 올랐다. 모든 품종 원두가 일제히 올랐지만 가장 심한 것은 로부스타 품종으로 지난 4월 초부터 500g 한 봉지 가격이 400원이나 뛰어올랐다”며 “소매가를 올리고 싶지만, 거래처 반발이 있으리라 예상해 올해를 지나고 내년부터 1000원 정도 올릴 예정이다”고 설명했다.\n",
        "원두가격 상승은 개인카페를 운영하는 자영업자 외에도 프랜차이즈 카페 가맹점주들에게도 걱정거리다. 프랜차이즈 카페의 경우 대량으로 원두를 구매해 어느 정도 소비자가격 방어가 가능하지만, 판매가격 조정은 온전히 본사의 결정이라 원두 납품가는 올랐는데 가격은 그대로여서 가맹점주 사이 볼멘소리도 나오고 있다.\n",
        "광주 북구와 남구에서 무인카페를 운영 중인 박모(32)씨는 “지난해 본사에서 원두값을 올렸지만 소비자 판매가는 조정해 주지 않아 상황이 어렵다”며 “올해 원두값이 치솟으면서 본사에서 또 납품가를 올릴 것으로 예상하지만 작년같이 판매가를 동결하게 되면 먹고 살길이 막막하다”고 토로했다.\n",
        "이미 가격을 올린 커피 브랜드들도 있다.\n",
        "커피 프랜차이즈인 하삼동커피, 더벤티, 커피빈 등이 원두, 설탕 등 원부자재 가격 상승과 임대료 및 인건비 영향으로 가격 인상을 감행했다.\n",
        "하삼동커피는 지난 1일부터 카페라떼 등 음료 6종의 가격을 200원씩 올렸다. 더벤티도 지난달 22일부터 카페라떼 등 음료 7종의 가격을 200원~500원 올렸다. 커피빈은 올해 초 카페라떼 등 우유가 포함된 음료의 가격을 200원씩 인상했으며 지난달 17일부터는 편의점에서 판매되는 파우치 음료 가격도 100원씩 인상됐다.'''\n"
      ],
      "metadata": {
        "id": "vWNIerx17Ry0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = article3   # 아래 분석에서는 input으로 사용"
      ],
      "metadata": {
        "id": "Qs-VObkbo0x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 뉴스 분야(주제, 장르) 분석: KPF-BERT-CLS 활용"
      ],
      "metadata": {
        "id": "ltP8G7Ke3Akj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 뉴스 분야(주제, 장르) 분석: BS_label로 분석. 나머지는 정확도 부족\n",
        "\n",
        "# 3개 classifier(big, small, region) label\n",
        "big_label = ['정치', '경제', '사회', '문화', '국제', '스포츠', 'IT_과학']\n",
        "small_label = ['국회_정당', '북한', '선거', '외교', '청와대', '행정_자치', '골프', '농구_배구', '야구_메이저리그',\n",
        "               '야구_일본프로야구', '올림픽_아시안게임', '축구_월드컵', '축구_한국프로축구', '축구_해외축구',\n",
        "               '교육_시험', '날씨', '노동_복지', '미디어', '사건_사고', '여성', '의료_건강', '장애인', '환경',\n",
        "               '미술_건축', '방송_연예', '생활', '요리_여행', '음악', '전시_공연', '종교', '출판', '학술_문화재',\n",
        "               '러시아', '미국_북미', '아시아', '유럽_EU', '일본', '중국', '중남미', '중동_아프리카', '국제경제',\n",
        "               '금융_재테크', '무역', '반도체', '부동산', '산업_기업', '서비스_쇼핑', '외환', '유통', '자동차',\n",
        "               '자원', '증권_증시', '취업_창업', '과학', '모바일', '보안', '인터넷_SNS', '콘텐츠']\n",
        "region_label = ['강원', '경기', '경남', '경북', '광주', '대구', '대전', '부산', '울산', '전남', '전북',\n",
        "                '제주', '지역일반', '충남', '충북']\n",
        "BS_label = {'정치': ['국회_정당', '북한', '선거', '외교', '정치일반', '청와대', '행정_자치'],\n",
        "             '스포츠': ['골프', '농구_배구', '스포츠일반', '야구_메이저리그', '야구_일본프로야구', '올림픽_아시안게임', '축구_월드컵', '축구_한국프로축구', '축구_해외축구'],\n",
        "             '사회': ['교육_시험', '날씨', '노동_복지', '미디어', '사건_사고', '사회일반', '여성', '의료_건강', '장애인', '환경'],\n",
        "             '문화': ['문화일반', '미술_건축', '방송_연예', '생활', '요리_여행', '음악', '전시_공연', '종교', '출판', '학술_문화재'],\n",
        "             '국제': ['국제일반', '러시아', '미국_북미', '아시아', '유럽_EU', '일본', '중국', '중남미', '중동_아프리카'],\n",
        "             '경제': ['경제일반', '국제경제', '금융_재테크', '무역', '반도체', '부동산', '산업_기업', '서비스_쇼핑', '외환', '유통', '자동차', '자원', '증권_증시', '취업_창업'],\n",
        "             'IT_과학': ['IT_과학일반', '과학', '모바일', '보안', '인터넷_SNS', '콘텐츠'],\n",
        "             '지역': ['강원', '경기', '경남', '경북', '광주', '대구', '대전', '부산', '울산', '전남', '전북', '제주', '지역일반', '충남', '충북']}\n",
        "big_label2id = {label: i for i, label in enumerate(big_label)}\n",
        "big_id2label = {i: label for label, i in big_label2id.items()}\n",
        "small_label2id = {label: i for i, label in enumerate(small_label)}\n",
        "small_id2label = {i: label for label, i in small_label2id.items()}\n",
        "region_label2id = {label: i for i, label in enumerate(region_label)}\n",
        "region_id2label = {i: label for label, i in region_label2id.items()}"
      ],
      "metadata": {
        "id": "dEF6XCtniWmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "import torch\n",
        "import statistics\n",
        "\n",
        "# 토크나이저와 모델 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained(\"jinmang2/kpfbert\")\n",
        "kpf_model1 = BertForSequenceClassification.from_pretrained(\"KPF/KPF-bert-cls1\")\n",
        "kpf_model2 = BertForSequenceClassification.from_pretrained(\"KPF/KPF-bert-cls2\")\n",
        "kpf_model3 = BertForSequenceClassification.from_pretrained(\"KPF/KPF-bert-cls3\")\n",
        "\n",
        "# Pipeline 설정\n",
        "small_cls = pipeline(\"text-classification\", model=kpf_model1, tokenizer=tokenizer)\n",
        "big_cls = pipeline(\"text-classification\", model=kpf_model2, tokenizer=tokenizer)\n",
        "region_cls = pipeline(\"text-classification\", model=kpf_model3, tokenizer=tokenizer)\n",
        "\n",
        "# 긴 텍스트를 적절한 길이의 문자열로 분할하는 함수: # 문장이 긴 경우, 분리해서 분석: KPF-BERT의 max_length = 512\n",
        "def split_long_text(text, max_length):\n",
        "    # 토크나이즈\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    # 최대 길이에 맞춰 토큰을 분할\n",
        "    for i in range(0, len(tokens), max_length - 2):  # [CLS]와 [SEP] 토큰을 위해 2를 뺌\n",
        "        chunk = tokens[i:i + max_length - 2]\n",
        "        chunk = ['[CLS]'] + chunk + ['[SEP]']  # [CLS]와 [SEP] 토큰을 추가\n",
        "        chunks.append(chunk)\n",
        "    # 각 청크에서 [CLS]와 [SEP] 토큰을 제거하고 텍스트를 다시 구성\n",
        "    reconstructed_texts = []\n",
        "    for chunk in chunks:\n",
        "        # [CLS]와 [SEP]를 제거\n",
        "        chunk = chunk[1:-1]\n",
        "        # 토큰을 텍스트로 변환\n",
        "        text = tokenizer.convert_tokens_to_string(chunk)\n",
        "        reconstructed_texts.append(text)\n",
        "    return reconstructed_texts\n",
        "\n",
        "# 실습\n",
        "#input = ''\n",
        "\n",
        "input_texts = split_long_text(input, max_length=512)  # max_length = 512\n",
        "results = [] # 분할된 텍스트로 예측 수행\n",
        "for text in input_texts:\n",
        "    # 각 파이프라인을 사용하여 예측\n",
        "    small_result = small_cls(text)[0]['label'].lstrip('LABEL_')\n",
        "    big_result = big_cls(text)[0]['label'].lstrip('LABEL_')\n",
        "    region_result = region_cls(text)[0]['label'].lstrip('LABEL_')\n",
        "    # 결과 저장\n",
        "    results.append([big_result, small_result, region_result])  # 분리된 각 text에 대해 출력\n",
        "small=small_id2label[int(statistics.mode([r[1] for r in results]))]   # small_result 값들 사이의 최빈값\n",
        "big=[big  for big, smalls in BS_label.items() if small in smalls][0]\n",
        "print([big, small])"
      ],
      "metadata": {
        "id": "gMu0HwIWpsAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 개체명 분석: KPF-BERT-NER 활용"
      ],
      "metadata": {
        "id": "NexnWBT125Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BmiCkIKTkn60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 드라이브마운트 필요: kpf_ner_label 파일(모듈)을 불러와 id2label 구성. id2label로 KPF-BERT-NER 분석에서 나타난 id를 label로 반환.\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "import sys\n",
        "import os\n",
        "directory_path = '/content/drive/MyDrive/2024_1_class/ai_coding/huggingface_github_models/' # kpf_ner_label.py가 있는 폴더\n",
        "sys.path.append(directory_path)\n",
        "import kpf_ner_label\n",
        "id2label, id2label=kpf_ner_label.kpf_ner_label()\n",
        "\n",
        "# 토크나이저와 모델 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained(\"jinmang2/kpfbert\")\n",
        "max_length = 512  # 모델의 최대 입력 길이\n",
        "model = BertForTokenClassification.from_pretrained(\"KPF/KPF-bert-ner\")\n",
        "\n",
        "# Pipeline 설정\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# 긴 텍스트를 적절한 길이의 문자열로 분할하는 함수: # 문장이 긴 경우, 분리해서 분석: KPF-BERT의 max_length = 512\n",
        "def split_long_text(text, max_length):\n",
        "    # 토크나이즈\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    # 최대 길이에 맞춰 토큰을 분할\n",
        "    for i in range(0, len(tokens), max_length - 2):  # [CLS]와 [SEP] 토큰을 위해 2를 뺌\n",
        "        chunk = tokens[i:i + max_length - 2]\n",
        "        chunk = ['[CLS]'] + chunk + ['[SEP]']  # [CLS]와 [SEP] 토큰을 추가\n",
        "        chunks.append(chunk)\n",
        "    # 각 청크에서 [CLS]와 [SEP] 토큰을 제거하고 텍스트를 다시 구성\n",
        "    reconstructed_texts = []\n",
        "    for chunk in chunks:\n",
        "        # [CLS]와 [SEP]를 제거\n",
        "        chunk = chunk[1:-1]\n",
        "        # 토큰을 텍스트로 변환\n",
        "        text = tokenizer.convert_tokens_to_string(chunk)\n",
        "        reconstructed_texts.append(text)\n",
        "    return reconstructed_texts\n",
        "\n",
        "# 개체명 분석 함수\n",
        "def kpf_ner_analysis(txt):\n",
        "    result = ner_pipeline(txt)\n",
        "    result1=[(int(n['entity'].replace('LABEL_','')), n['word']) for n in result]\n",
        "    result2=[(id2label[i], w) for i, w in result1 if i !=299]  # 개체명에 없는 '0' 단어 제외\n",
        "    result3 = []  # 합쳐진 단어를 저장할 리스트\n",
        "    for l, w in result2:\n",
        "        if w.startswith(\"##\"):  # 단어가 \"##\"으로 시작하는 경우\n",
        "            result3[-1][0] += w.lstrip(\"##\")  # 마지막으로 추가된 단어에 현재 단어를 합침\n",
        "        else:\n",
        "            result3.append([w,l.lstrip(\"B-\")])  # 그렇지 않은 경우, 단어를 리스트에 추가\n",
        "    return [(w,l) for [w,l] in result3]\n",
        "\n",
        "\n",
        "# 실습\n",
        "#input = '--'\n",
        "input_texts = split_long_text(input, max_length=512)  # max_length = 512\n",
        "results = []\n",
        "for txt in input_texts:\n",
        "    results.extend(kpf_ner_analysis(txt))\n",
        "\n",
        "ner_PS=','.join(set([w for w,l in results if 'PS_' in l]))  # 중복 등장 배제\n",
        "ner_OGG=','.join(set([w for w,l in results if 'OGG_' in l]))\n",
        "ner_LCP=','.join(set([w for w,l in results if 'LCP_' in l]))\n",
        "print(f'persons: {ner_PS}, organizations: {ner_OGG}, locations: {ner_LCP}')"
      ],
      "metadata": {
        "id": "AANVy4rakn3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한국어 요약: 허깅페이스 'gogamza/kobart-summarization'\n",
        "- KoBART 모델: BART(Bidirectional and Auto-Regressive Transformers)를 기반으로 한 한국어 뉴스 요약 모델.\n",
        "- BART는 transformer의 입력기와 출력기 모두 사용."
      ],
      "metadata": {
        "id": "jjS6ankzINTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
        "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
        "\n",
        "\n",
        "# 실습\n",
        "#input = '--'\n",
        "\n",
        "# 입력 아이디 생성\n",
        "raw_input_ids = tokenizer.encode(input, max_length=1024, truncation=True)  # 입력 텍스트의 최대 길이는 1024(이 모델의 입력과 출력 길이의 최댓값). 필요에 따라 적절하게 조정 가능.\n",
        "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "\n",
        "# 요약 생성\n",
        "summary_ids = model.generate(torch.tensor([input_ids]), max_length=100)  # max_length 조절 가능(위 참조)\n",
        "summary = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "Tk3iSwdnHIuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 혐오 글 분류1: 허깅페이스 'beomi/korean-hatespeech-multilabel'\n",
        "- beomi/KcELECTRA-base 활용 개발\n",
        "- 4개 클래스(bias_others(타인 차별)/ bias_gender(성차별)/offensive(공격적)/hate(혐오))에 대해 각각 True/False(확률 0.5 기준)로 이진 분류"
      ],
      "metadata": {
        "id": "2BRa8KBzfwgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "model_name = \"beomi/korean-hatespeech-multilabel\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# 실습\n",
        "input = '이 사람은 이슬람 테러리스트로 추방해야 함'\n",
        "\n",
        "# 토큰화 및 입력 아이디 생성\n",
        "inputs = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "# 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.sigmoid(outputs.logits)\n",
        "# 클래스별로 확률 반환과 이진분류(True/False) 변환\n",
        "classes = [\"bias_others\", \"bias_gender\", \"offensive\", \"hate\"] # bias_others(타인 차별)/ bias_gender(성차별)/offensive(공격적)/hate(혐오)\n",
        "result=[]\n",
        "for c, p in zip(classes, predictions.flatten().tolist()):\n",
        "    result.append((c,p, p>0.5))  # 각 클래스 값을 0.5(threshold) 기준으로 이진 분류로 변환.\n",
        "true_classes = [c for c, p, tf in result if tf == True]\n",
        "if not true_classes: true_classes = ['없음']\n",
        "print(result, f'\\n\\n유의하게 나타난(0.5 이상으로 True 판정) 클래스: {\", \".join(true_classes)}')"
      ],
      "metadata": {
        "id": "Wqx3SfwSOgJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 혐오 글 분류2: 허깅페이스 'sgunderscore/hatescore-korean-hate-speech'\n",
        "- https://github.com/sgunderscore/hatescore-korean-hate-speech\n",
        "- 혐오 분석: ['None', '기타 혐오', '남성', '단순 악플', '성소수자', '여성/가족', '연령', '인종/국적', '종교', '지역']"
      ],
      "metadata": {
        "id": "TkRurg9uSto9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline, BertForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "model_name = 'sgunderscore/hatescore-korean-hate-speech'\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pipe = TextClassificationPipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    device = -1, # gpu: 0\n",
        "    return_all_scores = True,\n",
        "    function_to_apply = 'sigmoid')\n",
        "\n",
        "\n",
        "# 실습\n",
        "input = '무슬림형제단 출신 A씨 속했던 단체, 각종 테러 일으켜'\n",
        "results=[tuple(r.values()) for r in pipe(input)[0]]\n",
        "highest=results[np.argmax([v for k,v in results])][0]\n",
        "pprint([results, f\"가장 강하게 나타난 클래스: {highest}\"])"
      ],
      "metadata": {
        "id": "2WMBlJcoOgGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대화 의도 분류: 허깅페이스 'gg4ever/intent-classification-korean'\n",
        "- fine-tuned for 'klue/roberta-base' used data : 'kor_3i4k'\n",
        "- Cho, W. I., Lee, H. S., Yoon, J. W., Kim, S. M., & Kim, N. S. (2018). Speech intention understanding in a head-final language: A disambiguation utilizing intonation-dependency. arXiv preprint arXiv:1811.04231.\n",
        "- 한국어 의도 분류를 위해 특화된 모델로, 텍스트 분류 작업에 적합\n",
        "- class: statement(진술), rhetorical question(수사적 질문), command(명령), intonation-dependent utterance(억양에 따라 달라지는 발화), rhetorical command(수사적 명령), question(질문), fragment(단편)"
      ],
      "metadata": {
        "id": "cdR8W5VpOtCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "auth_token=\"--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\"   # 허깅페이스의 access token. 허깅페이스에서 access token 만들어 반드시 자신의 것으로 교체 사용.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gg4ever/intent-classification-korean\", use_auth_token=auth_token)  # 인증 필요\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gg4ever/intent-classification-korean\", use_auth_token=auth_token)\n",
        "\n",
        "text_classifier = TextClassificationPipeline(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model.to('cpu'),\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "\n",
        "# 실습\n",
        "#input = '--'\n",
        "result = [tuple(d.values()) for d in text_classifier(input[:1000])[0]]  # input 길이가 대략 512개의 토큰을 넘지 않아야 함. 그래서 글자수 1000개로 제한했음. 문제 생기면 더 단축 필요.\n",
        "highest_class = result[np.argmax([b for a,b in result])][0]\n",
        "print(result, f'\\n\\n가장 강하게 나타난 클래스: {highest_class}')"
      ],
      "metadata": {
        "id": "J9U2q9q0OgEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}