{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1OjnwyAuArv0CoeosZPa861vRL0L6X5MH",
      "authorship_tag": "ABX9TyMJN2+dYnSjb48QCmiKrtYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonghhhh/bert_inference_042124/blob/main/colab_selenium_%EB%84%A4%EC%9D%B4%EB%B2%84%EC%B9%B4%ED%8E%98%EB%B8%94%EB%A1%9C%EA%B7%B8_032424.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzndw1K0SCaE"
      },
      "source": [
        "## selenium과 필요 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/yingfu46/scraping-a-dynamic-website-with-chrome-in-colab"
      ],
      "metadata": {
        "id": "vGuRhsshtuZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for running selenium in Google Colab\n",
        "## You don't need to run this code if you do it in Jupyter notebook, or other local Python setting\n",
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
        "pip install selenium"
      ],
      "metadata": {
        "id": "SkEeqwENq0ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromedriver-autoinstaller\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# set up the webdriver\n",
        "wd = webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "jKH2ljadq0Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 웹페이지에 접속하고 화면 하단까지 스크롤 다운하는 함수 만들기"
      ],
      "metadata": {
        "id": "hXhvjQT_LCqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def connect_scrolldown(target_url):\n",
        "    try:\n",
        "        wd.get(target_url)\n",
        "        # 페이지 끝까지 스크롤\n",
        "        last_height = wd.execute_script(\"return document.body.scrollHeight\") # JavaScript 실행, 웹 페이지의 전체 높이를 반환\n",
        "        while True:\n",
        "            # 페이지 끝까지 스크롤 다운\n",
        "            wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # 수평 스크롤 위치=0, 수직 스크롤 위치=document.body.scrollHeight(현재 페이지의 전체 높이=가장 하단)\n",
        "            # 페이지 로드를 기다림\n",
        "            time.sleep(2)  # 로딩 시간에 따라 조절이 필요할 수 있습니다.\n",
        "            # 새로운 스크롤 높이를 계산하고 마지막 스크롤 높이와 비교\n",
        "            new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "        print(\"scroll-down completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "CtCEi_tA56r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 검색 조건 설정하고 내용 수집하는 함수 만들기"
      ],
      "metadata": {
        "id": "Yx02CNU2LSrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import urllib.parse\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def navercafeblog_selenium_scrape(cafe=True, query='파이썬', date_from='20240301', date_to='20240302'):\n",
        "    encoded_query=urllib.parse.quote(query)\n",
        "    if cafe:\n",
        "        target_url=f\"https://search.naver.com/search.naver?ssc=tab.cafe.all&query={encoded_query}&cafe_where=articleg&st=rel&nso=so%3Ar%2Cp%3Afrom{date_from}to{date_to}\"\n",
        "    else:\n",
        "        target_url=f\"https://search.naver.com/search.naver?ssc=tab.blog.all&query={encoded_query}&cafe_where=articleg&st=rel&nso=so%3Ar%2Cp%3Afrom{date_from}to{date_to}\"\n",
        "    connect_scrolldown(target_url)\n",
        "    time.sleep(1)\n",
        "    html = wd.page_source\n",
        "    time.sleep(0.5)\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    time.sleep(0.5)\n",
        "    result_list=[]\n",
        "    try:\n",
        "        results = soup.select('div.view_wrap')\n",
        "        for result in results:\n",
        "            try: name=result.select('a.name')[0].text\n",
        "            except: name=''\n",
        "            try: title=result.select('a.title_link')[0].text\n",
        "            except: title=''\n",
        "            try: url=result.select('a.title_link')[0]['href']\n",
        "            except: url=''\n",
        "            try: date=result.select('span.sub')[0].text\n",
        "            except:date=''\n",
        "            try: text=result.select('a.dsc_link')[0].text\n",
        "            except: text=''\n",
        "            result_list.append([name,title,date,url,text])\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "    return pd.DataFrame(result_list, columns=['name','title','date','url','text'])"
      ],
      "metadata": {
        "id": "yWxC2_kUWqfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습\n",
        "result_df=navercafeblog_selenium_scrape(cafe=False, query='총선', date_from='20240301', date_to='20240320') #True= 네이버 카페, False=네이버 블로그, 관련도 순으로 1000여개 추출\n",
        "result_df.to_excel(\"--path/file.xlsx--\")"
      ],
      "metadata": {
        "id": "EJoi7wrH56pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}